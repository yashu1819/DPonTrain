{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "488c8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Simulation Constants ---\n",
    "TOTAL_CAPACITY = 90\n",
    "BASE_FARE = 1100.0\n",
    "BOOKING_DAYS = 60 # Booking window opens 60 days before departure\n",
    "\n",
    "# --- Baseline Scenario (No Dynamic Pricing) ---\n",
    "# As per your note, 75 seats are sold at a fixed price of 1100.\n",
    "BASELINE_SEATS_SOLD = 75\n",
    "BASELINE_REVENUE = BASELINE_SEATS_SOLD * BASE_FARE\n",
    "\n",
    "# --- Q-Learning Agent Hyperparameters ---\n",
    "LEARNING_RATE = 0.1 # Alpha\n",
    "DISCOUNT_FACTOR = 1 # Gamma\n",
    "EPSILON = 0.1 # Exploration rate\n",
    "\n",
    "# --- Reward Function Hyperparameter ---\n",
    "BETA = 10.0 # Penalty for price volatility in RVP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "1aa9752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RailwayEnvironment:\n",
    "    \"\"\"\n",
    "    Simulates the railway booking environment and customer demand.\n",
    "    The environment calculates demand based on a chosen price elasticity model\n",
    "    and returns the number of tickets sold as a Poisson distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, elasticity_model='power'):\n",
    "        self.elasticity_model = elasticity_model\n",
    "        \n",
    "        # Parameters for the elasticity models\n",
    "        if self.elasticity_model == 'linear':\n",
    "            # D(p) = a - b*p\n",
    "            # Set so that D(1100) is approx 75\n",
    "            self.a = 137  # Max demand at price 0\n",
    "            self.b = 0.114 # Sensitivity\n",
    "        elif self.elasticity_model == 'power':\n",
    "            # D(p) = a * p^(-b)\n",
    "            # Set so that D(1100) is approx 75\n",
    "            self.a = 90000 # Scaling factor\n",
    "            self.b = 1.2    # Price elasticity of demand\n",
    "        else:\n",
    "            raise ValueError(\"Invalid elasticity model. Choose 'linear' or 'power'.\")\n",
    "\n",
    "    def get_demand(self, price):\n",
    "        \"\"\"\n",
    "        Calculates the mean demand based on the price and model, then returns\n",
    "        a stochastic demand value from a Poisson distribution.\n",
    "        \"\"\"\n",
    "        if self.elasticity_model == 'linear':\n",
    "            # D(p) = a - b*p [cite: 114]\n",
    "            mean_demand = self.a - self.b * price\n",
    "        else: # Power model\n",
    "            # D(p) = a * p^(-b) [cite: 119]\n",
    "            mean_demand = self.a * (price ** -self.b)\n",
    "        \n",
    "        # Ensure demand is not negative\n",
    "        mean_demand = max(0, mean_demand)\n",
    "        \n",
    "        # Return a random demand based on the mean (Poisson distribution)\n",
    "        return np.random.poisson(mean_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4b074975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Implements the Q-learning algorithm to find the optimal pricing strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Action space: Percentage change from base fare \n",
    "        self.actions = [-0.15, -0.10, -0.05, 0.0, 0.05, 0.10, 0.20, 0.30]\n",
    "        # Initialize Q-table as a dictionary for sparse states\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.actions)))\n",
    "\n",
    "    def get_state(self, days_left, capacity_rem, velocity):\n",
    "        \"\"\"\n",
    "        Discretizes the continuous state variables into a manageable tuple.\n",
    "        This tuple will be used as a key in the Q-table.\n",
    "        \"\"\"\n",
    "        # Discretize days_left (e.g., into 10-day bins)\n",
    "        days_bin = days_left // 10\n",
    "        \n",
    "        # Discretize remaining capacity (e.g., into 10% bins)\n",
    "        capacity_bin = int((capacity_rem / TOTAL_CAPACITY) * 10)\n",
    "        \n",
    "        # Discretize booking velocity (e.g., low, medium, high)\n",
    "        if velocity < 5:\n",
    "            velocity_bin = 0 # Low\n",
    "        elif velocity < 15:\n",
    "            velocity_bin = 1 # Medium\n",
    "        else:\n",
    "            velocity_bin = 2 # High\n",
    "            \n",
    "        return (days_bin, capacity_bin, velocity_bin)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy strategy for action selection.\n",
    "        - With probability epsilon, explores a random action.\n",
    "        - With probability 1-epsilon, exploits the best-known action.\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < EPSILON:\n",
    "            return random.choice(self.actions)  # Explore\n",
    "        else:\n",
    "            action_idx = np.argmax(self.q_table[state])\n",
    "            return self.actions[action_idx]  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Updates the Q-value for the given state-action pair using the Bellman equation.\n",
    "        \"\"\"\n",
    "        action_idx = self.actions.index(action)\n",
    "        \n",
    "        old_value = self.q_table[state][action_idx]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-learning formula\n",
    "        new_value = old_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max - old_value)\n",
    "        self.q_table[state][action_idx] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "8ad55666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(reward_function_type, elasticity_model_type, num_episodes=3000):\n",
    "    \"\"\"\n",
    "    Runs the full Q-learning simulation with a fixed starting price on the first day.\n",
    "    \n",
    "    Args:\n",
    "        reward_function_type (str): 'SRM' or 'RVP'.\n",
    "        elasticity_model_type (str): 'linear' or 'power'.\n",
    "        num_episodes (int): Number of training iterations.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing final simulation results.\n",
    "    \"\"\"\n",
    "    env = RailwayEnvironment(elasticity_model=elasticity_model_type)\n",
    "    agent = QLearningAgent()\n",
    "\n",
    "    print(f\"Reward Function: {reward_function_type} | Elasticity Model: {elasticity_model_type}\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for episode in range(num_episodes):\n",
    "        capacity_rem = TOTAL_CAPACITY\n",
    "        last_24h_bookings = 0\n",
    "        prev_price = BASE_FARE\n",
    "\n",
    "        for day in range(BOOKING_DAYS, 0, -1):\n",
    "            # 1. Get State\n",
    "            velocity = last_24h_bookings\n",
    "            state = agent.get_state(day, capacity_rem, velocity)\n",
    "            \n",
    "            # 2. Choose Action - WITH NEW CONSTRAINT\n",
    "            if day == BOOKING_DAYS:\n",
    "                # On the first day, the price is always the base fare\n",
    "                action = 0.0 \n",
    "            else:\n",
    "                # From the second day onwards, the agent decides\n",
    "                action = agent.choose_action(state)\n",
    "            \n",
    "            price = BASE_FARE * (1 + action)\n",
    "            \n",
    "            # 3. Environment Responds\n",
    "            daily_demand = env.get_demand(price) / day \n",
    "            tickets_sold = min(daily_demand, capacity_rem)\n",
    "            \n",
    "            # 4. Calculate Reward\n",
    "            if reward_function_type == 'SRM':\n",
    "                reward = tickets_sold * price\n",
    "            else: # RVP\n",
    "                reward = (tickets_sold * price) - (BETA * abs(price - prev_price))\n",
    "            \n",
    "            # 5. Update State and Agent\n",
    "            capacity_rem -= tickets_sold\n",
    "            last_24h_bookings = tickets_sold\n",
    "            prev_price = price\n",
    "            \n",
    "            next_state = agent.get_state(day - 1, capacity_rem, last_24h_bookings)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            \n",
    "            if capacity_rem <= 0:\n",
    "                break\n",
    "        \n",
    "    # --- Evaluation Run (with exploitation only, epsilon=0) ---\n",
    "    capacity_rem = TOTAL_CAPACITY\n",
    "    last_24h_bookings = 0\n",
    "    total_revenue = 0\n",
    "    prices = []\n",
    "    daily_bookings = []\n",
    "\n",
    "    for day in range(BOOKING_DAYS, 0, -1):\n",
    "        velocity = last_24h_bookings\n",
    "        state = agent.get_state(day, capacity_rem, velocity)\n",
    "        \n",
    "        # Exploit the learned policy - WITH NEW CONSTRAINT\n",
    "        if day == BOOKING_DAYS:\n",
    "            action = 0.0 # Force base price on first day\n",
    "        else:\n",
    "            action_idx = np.argmax(agent.q_table[state])\n",
    "            action = agent.actions[action_idx]\n",
    "            \n",
    "        price = BASE_FARE * (1 + action)\n",
    "        \n",
    "        daily_demand = env.get_demand(price) / day\n",
    "        tickets_sold = min(daily_demand, capacity_rem)\n",
    "        \n",
    "        total_revenue += tickets_sold * price\n",
    "        capacity_rem -= tickets_sold\n",
    "        last_24h_bookings = tickets_sold\n",
    "        \n",
    "        prices.append(price)\n",
    "        daily_bookings.append(tickets_sold)\n",
    "        \n",
    "        if capacity_rem <= 0:\n",
    "            break\n",
    "            \n",
    "    # --- Compile and Return Results ---\n",
    "    prices.reverse()\n",
    "    total_seats_sold = TOTAL_CAPACITY - capacity_rem\n",
    "    occupancy = (total_seats_sold / TOTAL_CAPACITY) * 100\n",
    "    avg_price = total_revenue / total_seats_sold if total_seats_sold > 0 else 0\n",
    "    price_volatility = np.std(prices) if prices else 0\n",
    "    \n",
    "    results = {\n",
    "        \"Total Revenue\": total_revenue,\n",
    "        \"Occupancy (%)\": occupancy,\n",
    "        \"Seats Sold\": total_seats_sold,\n",
    "        \"Average Price\": avg_price,\n",
    "        \"Price Volatility\": price_volatility,\n",
    "        \"Prices\": prices\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b6b8c77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Function: SRM | Elasticity Model: linear\n",
      "Reward Model: SRM | Elasticity Model: linear\n",
      "Baseline Revenue (Fixed Price): ₹82,500.00\n",
      "Dynamic Pricing Revenue:        ₹84,177.50\n",
      "Seats Sold:                     90 / 90\n",
      "Occupancy:                      100.00%\n",
      "Average Ticket Price:           ₹935.31\n",
      "Price Volatility (Std Dev):     ₹21.48\n",
      "\n",
      "\n",
      "Reward Function: RVP | Elasticity Model: linear\n",
      "Reward Model: RVP | Elasticity Model: linear\n",
      "Baseline Revenue (Fixed Price): ₹82,500.00\n",
      "Dynamic Pricing Revenue:        ₹85,766.48\n",
      "Seats Sold:                     90 / 90\n",
      "Occupancy:                      100.00%\n",
      "Average Ticket Price:           ₹952.96\n",
      "Price Volatility (Std Dev):     ₹40.61\n",
      "\n",
      "\n",
      "Reward Function: SRM | Elasticity Model: power\n",
      "Reward Model: SRM | Elasticity Model: power\n",
      "Baseline Revenue (Fixed Price): ₹82,500.00\n",
      "Dynamic Pricing Revenue:        ₹94,108.22\n",
      "Seats Sold:                     73 / 90\n",
      "Occupancy:                      80.74%\n",
      "Average Ticket Price:           ₹1,295.10\n",
      "Price Volatility (Std Dev):     ₹181.30\n",
      "\n",
      "\n",
      "Reward Function: RVP | Elasticity Model: power\n",
      "Reward Model: RVP | Elasticity Model: power\n",
      "Baseline Revenue (Fixed Price): ₹82,500.00\n",
      "Dynamic Pricing Revenue:        ₹94,297.04\n",
      "Seats Sold:                     86 / 90\n",
      "Occupancy:                      95.76%\n",
      "Average Ticket Price:           ₹1,094.08\n",
      "Price Volatility (Std Dev):     ₹69.18\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run(REWARD_MODEL,ELASTICITY_MODEL ):\n",
    "    # --- Run the simulation ---\n",
    "    final_results = run_simulation(REWARD_MODEL, ELASTICITY_MODEL)\n",
    "\n",
    "    print(f\"Reward Model: {REWARD_MODEL} | Elasticity Model: {ELASTICITY_MODEL}\")\n",
    "    print(f\"Baseline Revenue (Fixed Price): ₹{BASELINE_REVENUE:,.2f}\")\n",
    "    print(f\"Dynamic Pricing Revenue:        ₹{final_results['Total Revenue']:,.2f}\")\n",
    "    print(f\"Seats Sold:                     {final_results['Seats Sold']:.0f} / {TOTAL_CAPACITY}\")\n",
    "    print(f\"Occupancy:                      {final_results['Occupancy (%)']:.2f}%\")\n",
    "    print(f\"Average Ticket Price:           ₹{final_results['Average Price']:,.2f}\")\n",
    "    print(f\"Price Volatility (Std Dev):     ₹{final_results['Price Volatility']:,.2f}\")\n",
    "    print(\"\\n\")\n",
    "run(\"SRM\",\"linear\" )\n",
    "run(\"RVP\", \"linear\")\n",
    "run(\"SRM\", \"power\")\n",
    "run(\"RVP\", \"power\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e293f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot(REWARD_MODEL,ELASTICITY_MODEL):\n",
    "#     final_results = run_simulation(REWARD_MODEL, ELASTICITY_MODEL)\n",
    "#     # --- Plotting the Price Fluctuation ---\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "#     plt.plot(range(len(final_results['Prices'])), final_results['Prices'], marker='o', linestyle='-', color='b')\n",
    "#     plt.title(f'Dynamic Price Over Booking Window ({REWARD_MODEL} | {ELASTICITY_MODEL})', fontsize=16)\n",
    "#     plt.xlabel('Days Before Departure', fontsize=12)\n",
    "#     plt.ylabel('Ticket Price (₹)', fontsize=12)\n",
    "#     plt.axhline(y=BASE_FARE, color='r', linestyle='--', label=f'Base Fare (₹{BASE_FARE})')\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.gca().invert_xaxis() # Show day 60 on the left and day 0 on the right\n",
    "#     plt.show()\n",
    "# plot(\"SRM\",\"linear\" )\n",
    "# plot(\"RVP\", \"linear\")\n",
    "# plot(\"SRM\", \"power\")\n",
    "# plot(\"RVP\", \"power\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
